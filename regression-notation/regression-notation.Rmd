---
title: "Regression Notation"
output: 
 learnr::tutorial:
    progressive: true
    allow_skip: true
    css:
      - www/bootstrap.min.css
      - www/flexdashboard.min.css
      - www/style.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(jpeg)
library(psych)
library(HistData)
library(manipulate)
library(UsingR)


tutorial_options(exercise.startover = TRUE,
                 exercise.eval = TRUE, 
                 exercise.cap = "R Code",
                 exercise.completion = TRUE,
                 exercise.diagnostics = TRUE)

options(digits = 3, scipen = 9999)

```

#### ALEx

Welcome to the <a href="../../index" target="_blank">**A**rcus **L**earning **Ex**change</a>, or <a href="../../index" target="_blank">ALEx</a>, a service of Children's Hospital of Philadelphia. 

Check out our <a href="../../catalog" target="_blank">catalog</a> for more lessons! 

## Some Basic Definitions

It is a good idea to have some agreement about notation. 

```{r notation, echo = FALSE}
knitr::include_graphics("www/notation.png", 
                          auto_pdf = getOption("knitr.graphics.auto_pdf", 
                                                FALSE), dpi = NULL)
```


Therefore, in this lesson, we'll cover some basic definitions and notation used throughout the ALEx lessons about regression. 

We will try to minimize the amount of mathematics required for this class.

No calculus is required. 


## Notation for Data

* We write $X_1, X_2, \ldots, X_n$ to describe $n$ data points.
* As an example, consider the data set $\{1, 2, 5\}$. Then 
  * $X_1 = 1$
  * $X_2 = 2$
  * $X_3 = 5$
  * $n = 3$
* We often use a different letter than $X$, such as $Y_1, \ldots , Y_n$.
* We will typically use **Greek letters** for things we don't know. 
  For instance, $\mu$ is a mean that we'd like to estimate.
* We will use **capital letters** for conceptual values of the variables and **lowercase letters** for realized values.
  * So this way we can write $P(X_i > x)$ ("the probability of a variable $\displaystyle X$ being greater than an $\displaystyle x$ that we are looking at right now")
  * $X_i$ is a conceptual random variable.
  * $x$ is a number that we plug in. It might be a 2, or a -100. But it is specific.


## The Mean

* Define the empirical mean as
$$
\bar X = \frac{1}{n}\sum_{i=1}^n X_i. 
$$

* The giant sigma indicates to sum all values from what's under it to what's above it. Here, that means that x-bar, or the mean, is the summation of each element of the variable $\displaystyle X$ (all elements from the first, where $\displaystyle n = 1$, to the last, where $\displaystyle n = n$ or the total number of elements in the variable), divided by $\displaystyle n$. 

* Notice if we subtract the mean from data points, we get data that has mean 0. That is, if we define
$$
\tilde X_i = X_i - \bar X.
$$
then the mean of the $\tilde X_i$ is 0.
* This process is called "centering" the random variables.
* The mean is a measure of central tendency of the data.
* The mean is 
  the least squares solution for minimizing
  $$
  \sum_{i=1}^n (X_i - \mu)^2
  $$



## The Empirical Standard Deviation and the Variance

* Define the empirical variance as 
$$
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2 
= \frac{1}{n-1} \left( \sum_{i=1}^n X_i^2 - n \bar X ^ 2 \right)
$$
* The empirical standard deviation is defined as
$S = \sqrt{S^2}$. Notice that the standard deviation has the same units as the data.
* The data defined by $X_i / s$ have empirical standard deviation 1. This is called "scaling" the data.
* The empirical standard deviation is a measure of spread.
* Sometimes people divide by $n$ rather than $n-1$ (the latter
produces an unbiased estimate.)


## Normalization

* The the data defined by
$$
Z_i = \frac{X_i - \bar X}{s}
$$
have empirical mean zero and empirical standard deviation 1. 
* The process of centering then scaling the data is called "normalizing" the data. 
* Normalized data are centered at 0 and have units equal to standard deviations of the original data. 
* Example, a value of 2 form normalized data means that data point
was two standard deviations larger than the mean.


## The Empirical Covariance
* Consider now when we have pairs of data, $(X_i, Y_i)$.
* Their empirical covariance is 
$$
Cov(X, Y) = 
\frac{1}{n-1}\sum_{i=1}^n (X_i - \bar X) (Y_i - \bar Y)
= \frac{1}{n-1}\left( \sum_{i=1}^n X_i Y_i - n \bar X \bar Y\right)
$$
* Some people prefer to divide by $n$ rather than $n-1$ (the latter
produces an unbiased estimate.)
* The correlation is defined is
$$
Cor(X, Y) = \frac{Cov(X, Y)}{S_x S_y}
$$
where $S_x$ and $S_y$ are the estimates of standard deviations 
for the $X$ observations and $Y$ observations, respectively.


## Some Facts about Correlation
* $Cor(X, Y) = Cor(Y, X)$
* $-1 \leq Cor(X, Y) \leq 1$
* $Cor(X,Y) = 1$ and $Cor(X, Y) = -1$ only when the $X$ or $Y$ observations fall perfectly on a positive or negative sloped line, respectively.
* $Cor(X, Y)$ measures the strength of the linear relationship between the $X$ and $Y$ data, with stronger relationships as $Cor(X,Y)$ heads towards -1 or 1.
* $Cor(X, Y) = 0$ implies no linear relationship. 

## Contributions

Slides to accompany this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses.

[Brian Caffo, Jeff Leek, Roger Peng, Nick Carchedi, Sean Kross,](https://github.com/DataScienceSpecialization/courses#contributors) and Sheila Braun


## License

These materials are available under the Creative Commons Attribution NonCommercial ShareAlike (CC-NC-SA) license (http://www.tldrlegal.com/l/CC-NC-SA).