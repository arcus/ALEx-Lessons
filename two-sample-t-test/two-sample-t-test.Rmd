---
title: "Two-Sample _t_ Test"
output: 
 learnr::tutorial:
    progressive: true
    allow_skip: true
    css:
      - www/bootstrap.min.css
      - www/flexdashboard.min.css
      - www/style.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
options(digits = 3, scipen = 9999)
library(learnr)
library(lsr)
library(FSA)
library(lattice)
library(ggplot2)
library(rcompanion)
library(pwr)
tutorial_options(exercise.startover = TRUE,
  exercise.eval = TRUE, 
  exercise.cap = "R Code",
  exercise.completion = TRUE,
  exercise.diagnostics = TRUE)
  
this_Input =("
Group Value
2pm    69    
2pm    70    
2pm    66    
2pm    63    
2pm    68    
2pm    70    
2pm    69    
2pm    67    
2pm    62    
2pm    63    
2pm    76    
2pm    59    
2pm    62    
2pm    62    
2pm    75    
2pm    62    
2pm    72     
2pm    63    
5pm    68
5pm    62
5pm    67
5pm    68
5pm    69
5pm    67
5pm    61
5pm    59
5pm    62
5pm    61
5pm    69
5pm    66
5pm    62
5pm    62
5pm    61
5pm    70
")
ht = read.table(textConnection(this_Input),header=TRUE)
names(ht) <- c("section", "inches")
tt_height <- t.test(inches ~ section, 
                    data =ht,
                    var.equal = TRUE,
                    conf.level = 0.95)
p_height <- round(tt_height$p.value, 2)

library(lsr)
ht_d <- cohensD(inches ~ section, data = ht, method  = "unequal")
p_height <- round(tt_height$p.value, 2)
```

#### ALEx

Welcome to the <a href="../../index" target="_blank">**A**rcus **L**earning **Ex**change</a>, or <a href="../../index" target="_blank">ALEx</a>, a service of Children's Hospital of Philadelphia. 

Check out our <a href="../../catalog" target="_blank">catalog</a> for more lessons! 

## The Purpose of this Lesson

This lesson is both a reference and a tutorial. It contains all the code samples you need to perform a two-sample _t_ test on your own data as well as background information such as how to interpret output and which graphs you might want to use to demonstrate results (and in some cases which graphs _not_ to use). 

You can also learn to understand and perform a two-sample _t_ test by working through the lesson from start to finish. 

## What kinds of Variables work with a _t_ Test

Studentâ€™s t test for two samples shows whether the means of the measurement variable are significantly different in two groups. Therefore you need these variables:

>* One [scalar (a.k.a. "measurement") variable](http://www.biostathandbook.com/variabletypes.html#measurement)     
* One [nominal variable](http://www.biostathandbook.com/variabletypes.html) with only two categories or two categories that you have selected. 

You can also use a binary variable here as it is functionally a nominal variable with two categories. 

A Student's _t_ test is mathematically identical to a one-way ANOVA with two categories; but because comparing the means of two samples is such a common experimental design, and because the _t_ test is familiar to many more people than the ANOVA, we treat the two-sample _t_ test separately.

```{r a, echo=FALSE}
question("What other test does the two-sample _t_ test NOT resemble?",
         answer("a one-sample _t_ test"),
         answer("a one-way ANOVA with only two categories"),
         answer("a paired _t_ test"),
         answer("a linear regression model", correct = TRUE),
         allow_retry = TRUE,
  random_answer_order = TRUE
)
```

## Null and Alternative Hypotheses

>* **H<sub>0</sub>**: The means of the measurement variable are equal in the two groups.     
* **H<sub>A</sub>** (2-sided): The means of the measurement variable are not equal in the two groups.     
* **H<sub>A</sub>** (1-sided): The mean of the measurement variable is higher (specifically---or lower specifically, but not either-or because that would be a 2-sided test) in one of the groups.

```{r bb, echo=FALSE}
question("Which are examples of 1-sided null hypotheses?",
         answer("It usually takes you less time than it takes me to make pie dough", correct = TRUE),
         answer("The average monthly electric bill in Philadelphia, PA is different from the average monthly electric bill in Burlington, VT"),
         answer("The average monthly electric bill in Philadelphia, PA is less than what it is in Burlington, VT", correct = TRUE),
         answer("The average monthly electric bill in Philadelphia, PA is more than what it is in Burlington, VT", correct = TRUE),
         allow_retry = TRUE,
         random_answer_order = TRUE
)
```

## Examples of the Null and Alternative Hypotheses

You have become curious about hand sizes. Are right hands the same size as left hands?

There are two variables in that question. 

```{r two-vars, echo=FALSE}
question("What are the two variables?",
         answer("right hands and left hands"),
         answer("hand size and type of person (politician vs. lumberjack)"),
         answer("hand size and hand type (left vs. right)", correct = TRUE),
         allow_retry = TRUE,
  random_answer_order = TRUE
)
```

In fall 2004, students in the 2 p.m. section of John McDonald's Biological Data Analysis class had an average height of 66.6 inches, while the average height in the 5 p.m. section was 64.6 inches. Are the average heights of the two sections significantly different? 

First, let's look at the data. It's been loaded in `ht` for you. Use `head` to look at it.

```{r heightnames, exercise = TRUE, exercise.lines=7}

```
```{r heightnames-solution}
head(ht)
```

The null hypothesis and alternatives are, therefore

>* **Example H<sub>0</sub>**: "Mean heights in the two sections are the same."     
* **Example H<sub>A</sub>** (2-sided): "Mean heights in the two sections are not the same."     
* **Example H<sub>A</sub>** (1-sided, version 1): "Mean height in the 2 pm section is higher than in the 5 pm section."     
* **Example H<sub>A</sub>** (1-sided, version 2): "Mean height in the 5 pm section is higher than in the 2 pm section."     

## Background

There are several statistical tests that use the _t_ distribution ([What's a distribution?](https://www.khanacademy.org/math/statistics-probability/displaying-describing-data/comparing-features-distributions/v/comparing-distributions?modal=1)) and can be called _t_ tests. One is Student's _t_ test for two samples, named after "Student," the pseudonym William Gosset used to hide his employment by the Guinness brewery in the early 1900s (they had a rule that their employees weren't allowed to publish, and Guinness didn't want other employees to know that they were making an exception for Gosset). Student's _t_ test for two samples compares the means in two groups.

```{r aa, echo=FALSE}
question("What might have been one of Student's favorite pasttimes?",
      answer("Drinking beer", correct = TRUE),
      answer("Working as a hairdresser"),
      answer("Flying to Jupiter's moons"),
      answer("Going to school full time"),
         allow_retry = TRUE
)
```

## Assumptions

Every statistical test relies upon assumptions about the data you give it. The next few sections are about the assumptions upon which a two-sample _t_ test rests. Note that the _t_ test is fairly reliable even when assumptions are violated, within reason. 

### Normality

The _t_ test assumes that the observations _within each group_ (as opposed to all of the observations together before you broke them into groups) are normally distributed. 

Fortunately, it is not at all sensitive to deviations from this assumption if the distributions of the two groups are the same (if both distributions are skewed to the right, for example). 

[John McDonald wrote](http://www.biostathandbook.com/twosamplettest.html) that he had done simulations with a variety of non-normal distributions, including flat, bimodal, and highly skewed, and the two-sample _t_ test always gave about 5% false positives, even with very small sample sizes. If your data is severely non-normal, you should try to find a data transformation that makes it more normal; but don't worry if you can't find a good transformation or don't have enough data to check the normality.

```{r bad_distributions, echo=FALSE}
question("Which of the following situations needs to be true for the same data set to result in too many false positives from a two-sample _t_ test? (Please select ALL that apply)",
         answer("A very small sample size", correct = TRUE),
         answer("A flat distribution in one group", correct = TRUE),
         answer("A bimodal distribution in one group", correct = TRUE),
         allow_retry = TRUE,
         random_answer_order = TRUE
)
```

If your data is severely non-normal, _and_ you have different distributions in the two groups (one data set is skewed to the right and the other is skewed to the left, for example), _and_ you have small samples (less than 50 or so), then the two-sample _t_ test can give inaccurate results, with considerably more than 5% false positives. A data transformation won't help you here, and neither will a Mann-Whitney U-test. It would be pretty unusual in biology to have two groups with different distributions but equal means, but if you think that's a possibility, you should require a _p_ value much less than 0.05 to reject the null hypothesis.

```{r tt_perfect_storm, echo=FALSE}
question("Which of the following is NOT a necessary aspect of a distribution that will cause a _t_ test to result in too many false positives?",
         answer("Small sample size"),
         answer("Non-normality"),
         answer("Two groups with skewness in opposite directions"),
         answer("Both negative and positive numbers", correct = TRUE),
         allow_retry = TRUE,
         random_answer_order = TRUE
)
```

```{r tt_tf_assumptions, echo=FALSE}
question("True or false: You can usually still perform a reliable _t_ test if only two out of three of the conditions in the previous question have been met.",
         answer("TRUE", correct = TRUE),
         answer("FALSE"),
         allow_retry = TRUE,
         random_answer_order = TRUE
)
```

### Homoscedasticity

In addition to normality, the two-sample _t_ test also assumes homoscedasticity (equal variances in the two groups). If you have a balanced design (equal sample sizes in the two groups), the test is not very sensitive to heteroscedasticity unless the sample size is very small (less than 10 or so); the standard deviations in one group can be several times as big as in the other group, and you'll get _p_ < 0.05 about 5% of the time if the null hypothesis is true. 

With an unbalanced design, heteroscedasticity is a bigger problem; if the group with the smaller sample size has a bigger standard deviation, the two-sample _t_ test can give you false positives much too often. If your two groups have standard deviations that are substantially different (such as one standard deviation is twice as big as the other) and your sample sizes are small (less than 10) or unequal, you should use Welch's _t_ test instead.

```{r unbalanced, echo=FALSE}
question("What can cause a two-sample _t_ test to become more sensitive to the assumption of homoscedasticity?",
         answer("Heteroscedasticity"),
         answer("A very small sample (< 10 or so) and unequal sample sizes in the two groups", correct = TRUE),
         answer("Skewness in the data"),
         answer("Unequal sample sizes in the two groups"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

```{r belch_for_welches, echo=FALSE}
question("What should you do if your two groups have standard deviations that are substantially different AND your sample sizes are small OR unequal",
         answer("Drink Welch's grape juice"),
         answer("Perform a squelch test instead of a _t_ test"),
         answer("Use linear regression instead of a _t_ test"),
         answer("Perform a Welch's _t_ test instead of the regular two-sample _t_ test", correct = TRUE),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

## _t_ Test Prep

I would love to have been able to plunk a code chunk here for you so you could perform this test without being confused with more facts than you want. You can select "Perform the _t_ Test" from the table of contents on the left side of this window if you want to keep things simple. And most of the time--especially with a _t_ test--you'll be fine if you do that. 

However, you have a reputation to protect (as do I), which means you need to make sure that all of this is true:

* you haven't made any faulty assumptions about the data that would render your analysis meaningless    
* you have established what a meaningful effect size is in the real world    
* you have sufficient power to find an effect if one exists    
* once you have run the test, you can understand the output

That's why the prep section of this lesson isn't just a few lines of code.

```{r to_skip_or_not_to_skip, echo=FALSE}
question("What do you want to do?",
         answer("Skip to 'Perform the Analysis'"),
         answer("Find out how to do ethical research and set myself up for success", correct = TRUE),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```


### First Test the Assumptions


```{r prepare-coach_ttest}
Input = ("
Instructor       Student  Sodium
'Brendon Small'  a        1200
'Brendon Small'  b        1400
'Brendon Small'  c        1350
'Brendon Small'  d         950
'Brendon Small'  e        1400
'Brendon Small'  f        1150
'Brendon Small'  g        1300
'Brendon Small'  h        1325
'Brendon Small'  i        1425
'Brendon Small'  j        1500
'Brendon Small'  k        1250
'Brendon Small'  l        1150
'Brendon Small'  m         950
'Brendon Small'  n        1150
'Brendon Small'  o        1600
'Brendon Small'  p        1300
'Brendon Small'  q        1050
'Brendon Small'  r        1300
'Brendon Small'  s        1700
'Brendon Small'  t        1300
'Coach McGuirk'  u        1100
'Coach McGuirk'  v        1200
'Coach McGuirk'  w        1250
'Coach McGuirk'  x        1050
'Coach McGuirk'  y        1200
'Coach McGuirk'  z        1250
'Coach McGuirk'  aa       1350
'Coach McGuirk'  ab       1350
'Coach McGuirk'  ac       1325
'Coach McGuirk'  ad       1525
'Coach McGuirk'  ae       1225
'Coach McGuirk'  af       1125
'Coach McGuirk'  ag       1000
'Coach McGuirk'  ah       1125
'Coach McGuirk'  ai       1400
'Coach McGuirk'  aj       1200
'Coach McGuirk'  ak       1150
'Coach McGuirk'  al       1400
'Coach McGuirk'  am       1500
'Coach McGuirk'  an       1200
")

coach = read.table(textConnection(Input),header=TRUE)
rm(Input)
names(coach) <- tolower(names(coach))
coach_d <- cohensD(sodium ~ instructor, data = coach)
```

Let's summarize the `coach` data by group. Use the `FSA` package, the function `Summarize`, the formula `sodium ~ instructor`, and 3-digit output. If you don't know how to arrange what you pass to `Summarize`, press `tab` while typing between the parentheses to see what parameters you can set. If that doesn't help, click on Solution for a (very broad) hint.

```{r coach-summary, exercise=TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r coach-summary-solution}
library(FSA)
Summarize(sodium ~ instructor,
          data = coach,
          digits = 3)
```

To check the normality assumptions, we want to see histograms by group (by instructor, in this case). The `right = FALSE` option is used with the Coach McGuirk data to improve the appearance of the plot in this particular case. Follow these steps:

1. create a dataset called `Brendon` that contains all elements of `coach$sodium` where `coach$instructor` is "Brendon Small". That's tricky syntax if you aren't used to base R selection, so I'll give it to you. `Brendan` should get `data$sodium[data$instructor == "Brendon Small"]`.    
2. Create another data set, `McGuirk`, using the same syntax, but this time match on "Coach McGuirk".    
3. Make sure `rcompanion` has been loaded in the library.
4. Use `plotNormalHistogram`, passing it the `Brendon` data set.    
5. Now do the same thing with the other data set, passing also the parameter `right` equal to `FALSE`.

```{r coach-hist, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r coach-hist-solution}
Brendon = coach$sodium[coach$instructor == "Brendon Small"]
McGuirk = coach$sodium[coach$instructor == "Coach McGuirk"]
library(rcompanion)
plotNormalHistogram(Brendon)
plotNormalHistogram(McGuirk, right = FALSE)
```

You could use `lattice` to do the same thing with more information, but adding normal curves to histograms by groups requires some extra code. 

1. Make sure `lattice` is in the library.    
2. Call the function `histogram`. Use the formula `~ sodium | instructor`, data is `coach`, `type` of plot is "density", `layout` is `c(1, 2)` (1 column, 2 rows).  

See what happens with that code.

```{r lattice-hists, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r lattice-hists-solution}
library(lattice)
histogram(~ sodium | instructor, 
          data = coach,
          type = "density",
          layout = c(1, 2))
```

Now add to your code. Create a function called `panel`. This is a bit complex, and we're not here for a `lattice` lesson, so go ahead and get the code from the Solution button. Note that you write a function within the call to `histogram`.

```{r lattice-normal-curves, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r lattice-normal-curves-solution}
library(lattice)
histogram(~ sodium | instructor, 
          data = coach,
          type   = "density", 
          layout = c(1,2), ### columns and rows of individual plots
          panel = function(x, ...) {
            panel.histogram(x, ...)
            
            panel.mathdensity(dmath = dnorm, 
                              col   = "blue",
                              lwd   = 2,
                              args  = list(mean=mean(x),
                                           sd=sd(x)), ...)})
```

A density plot doesn't use the frequency with which values appear, but rather proportions of values throughout the distribution. 

Now for some box plots for the data by group. Call the function `boxplot` and pass it the formula `sodium ~ instructor` using `data = coach`.

```{r box-coach, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r box-coach-solution}
boxplot(sodium ~ instructor, data = coach)
```

Based on the output, we can say the data is normal enough for an unpaired Student's _t_ test.

## Effect Size: How much of a Difference do we Care About?

The effect size estimation is calculated on the basis of the groups means and standard deviations in the case of a two-sample _t_ test. 

### Identify a Meaningful Effect Size: Calculate Cohen's _d_

Calculate Cohen's _d_ for the height data. Put the package `lsr` in the library first as you do for the one-sample _t_ test. However, when you have two means in the same data set, the syntax is different. 

Pass `cohensD` the formula `inches ~ section`, then set the `data` parameter equal to `ht`, then, because the two groups in our dataset have different counts, set the `method` parameter equal to "unequal". Assign the output to a new variable, `ht_d`, then put parentheses around the entire statement so you can see the output even though it is being assigned to a variable. 

Then do the same thing for the sodium data `coach`. The formula concerns `sodium` and `instructor`. You don't have to specify a method because the two groups have the same count.

```{r coach-cohen, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r coach-cohen-solution}
library(lsr)
(ht_d <- cohensD(inches ~ section, data = ht, method  = "unequal"))
(coach_d <- cohensD(sodium ~ instructor, data = coach))
```

### Interpret Cohen's _d_

Cohen's _d_ ranges from 0 to $\normalsize \infty$, with 0 indicating no effect: the means are equal. Cohenâ€™s _d_ can be positive or negative depending on whether one mean is greater than or less than the other. It is closely related to the standard deviation. 

We use Cohen's _d_ to _standardize_ the effect size so we can compare the importance of the difference in height between the two sections with the importance of the difference between, say, distances between pairs of stars. We use Cohen's _d_ so scale does not confuse us. What's big or small in one situation is perhaps not so much in another situation.
 
A Cohenâ€™s _d_ of 0.5 suggests that the means differ by one-half the standard deviation of the data. A Cohenâ€™s _d_ of 1.0 suggests that the means differ by one standard deviation of the data.

Rule of thumb about effect sizes: 

* Small effect = 0.2    
* Medium effect = 0.5    
* Large effect = 0.8    

Look at `ht_d` and `coach_d`. 

```{r ht_d_lookat, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r ht_d_lookat-solution}
ht_d
coach_d
```


```{r ht-effect-size, echo=FALSE}
question("What would you say the effect size is between heights based on the rule of thumb above?",
         answer("Small"),
         answer("Medium", correct = TRUE),
         answer("Large"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

```{r coach-effect-size, echo=FALSE}
question("What would you say the effect size is between sodium levels based on the rule of thumb above?",
         answer("Small", correct = TRUE),
         answer("Medium"),
         answer("Large"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

## Power Analysis

To estimate the sample sizes needed to detect a significant difference between two means, you need the following:

* **Alpha** ($\alpha$), or the significance level (usually 0.05)
* **Beta** ($\beta$), the probability of a false positive: accepting the null hypothesis when it is false. Common values for $\beta$ are 0.50, 0.80 and 0.90.
* **The ratio of one sample size to the other**. The most powerful design is to have equal numbers in each group such that $\frac{N1}{N2} = 1$, but sometimes it's easier to get large numbers of one of the groups. For example, if you're comparing the bone strength in mice that have been reared in zero gravity aboard the International Space Station vs. control mice reared on earth, you might decide ahead of time to use three control mice for every one expensive space mouse such that $\frac{N1}{N2} = 3$.

The `pwr` package has a function `pwr.2p2n.test`. You pass it the following, leaving exactly 1 item set to `NULL` so it can be calculated from the others. In our use cases, we want to find out `power`.

* `h` is effect size    
* `n1` is the number of observations in the first sample    
* `n2` is the number of observations in the second sample    
* `sig.level` is the significance level you have chosen for the test    
* `power` is the power of the test you would like (many people choose .80)
* `alternative` is one of    
    * "two.sided"
    * "greater"
    * "less"
    
Run `pwr.2p2n.test` for `ht` after `Summarize` of `inches ~ section` shows you the counts. Make sure that `pwr` is in the library. Use `?` to find out more about `Summarize` or `pwr.2p2n.test`. 

```{r pwrht, exercise = TRUE, exercise.lines = 8}

```
```{r pwrht-solution}
library(pwr)
Summarize(inches ~ section, data = ht)
pwr.2p2n.test(h = ht_d, 
              n1 = 18,
              n2 = 16,
              sig.level = .05,
              power = NULL,
              alternative = "two.sided")
```

Now do the same for the `coach` data.

```{r coachpwr, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r coachpwr-solution}
library(pwr)
Summarize(sodium ~ instructor, data = coach)
pwr.2p2n.test(h = ht_d, 
              n1 = 20,
              n2 = 20,
              sig.level = .05,
              power = NULL,
              alternative = "two.sided")
```

Power is quite low in both cases. How much data do we need to have adequate power? Run the same analyses you did before using `ht`, but this time set `n2` to `NULL` and ask for `power = .80`. Guess at `n1`.  

```{r power80, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r power80-solution}
pwr.2p2n.test(h = ht_d, 
              n1 = 40,
              n2 = NULL, 
              sig.level = .05,
              power = .80,
              alternative = "two.sided")
```

Now adjust `n1` and see if the situation improves.

```{r power802, exercise = TRUE, exercise.lines = 8, exercise.setup = "prepare-coach_ttest"}

```
```{r power802-solution}
pwr.2p2n.test(h = ht_d, 
              n1 = 79,
              n2 = NULL, 
              sig.level = .05,
              power = .80,
              alternative = "two.sided")
```

Now you can perform the _t_ test with a good sense of how meaningful any results will be.


## Perform the _t_ Test

We'll use the aptly named function `t.test` to see if the means are NOT the same in the two sections. Pass it the formula `inches ~ section`, then `data = ` the data set name, then two parameters: `alternative` set to `two.sided` and `conf.level` set to the usual 0.95.


```{r height-t-test, exercise = TRUE, exercise.lines = 15}

```
```{r height-t-test-solution}
t.test(inches ~ section, # the nominal variable with two options
       data =ht,    # the measurement or scalar variable
       alternative = "two.sided", 
       conf.level = 0.95)
```

## Interpret the Output

According to the output, _p_ is `r p_height`. We would reject the null hypothesis if _p_ < .05 (the usual standard [but don't get me started]). In this case, it's not. We cannot reject the null hypothesis that mean heights in the two groups are the same. 

>Note: We have not proven that the null hypothesis is true, just that we can't say it's _not_ true. 

To report the results of a two-sample _t_ test, you would provide the test statistic (_t_), the [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) (df), and the _p_ value (_p_) in a sentence like this: "based on the results of a _t_ test (_t_ = `r round(tt_height$statistic[[1]], 2)`, df = `r round(tt_height$parameter[[1]], 2)`, _p_ = `r round(tt_height$p.value[[1]], 2)`), we do not reject the null hypothesis that the two group means are the same." To be truly thorough, provide also the [confidence interval](https://www.khanacademy.org/math/statistics-probability/displaying-describing-data/comparing-features-distributions/v/comparing-distributions?modal=1).

### More about the Output

Run `t.test` again on the same data, but this time store the result in `tt_height`. 

```{r prepare-height_ttest}
tt_height <- t.test(inches ~ section, 
                    data =ht,
                    var.equal = TRUE,
                    conf.level = 0.95)
```
```{r heigh_ttest, exercise=TRUE, exercise.lines=15}

```
```{r heigh_ttest-solution}
tt_height <- t.test(inches ~ section, # the nominal variable with two options
                    data =ht,    # the measurement or scalar variable
                    alternative = "two.sided", 
                    conf.level = 0.95)
```

In R, everything is an object. Use `str` (short for "structure"--and so many other things, but here all we care about is "structure") to find out what the output looks like to R. Pass `str` the object you just created.

```{r object_appearance, exercise = TRUE, exercise.lines=7}

```
```{r object_appearance-solution}
str(tt_height)
```

That's interesting. What looked like a paragraph of output was actually a list! Try accessing elements of the list, for instance, `method`.

```{r access_method, exercise = TRUE, exercise.lines=7}

```
```{r access_method-solution}
tt_height$method
```

You can use output very nicely in your [reproducible Markdown text](https://education.arcus.chop.edu/scripted-analysis/) by calling it within text while you write a Markdown document. For instance, you might write, "I just did a " and then type a tic-mark ("\`", likely your lowercase "~", in the upper left corner of your keyboard, not to be confused with a single quote), then the text "r tt_height$method" without quotation marks, and finally, a closing tic-mark. The rendered (knitted) output would be 

>I just did a `r tt_height$method`. 

Let's go through the _t_ test outputted ("put out"?) list one by one.

### Statistic

Access `statistic` within `tt_height`.

```{r access_statistic, exercise = TRUE, exercise.lines=7}

```
```{r access_statistic-solution}
tt_height$statistic
```



```{r statistic_output, echo=FALSE}
question("What is the value of statistic?",
         answer("66.55556"),
         answer("1.29", correct = TRUE),
         answer("31.17529"),
         answer("0"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```



```{r statistic_name, echo=FALSE}
question("What's it called?",
         answer("_p_"),
         answer("df"),
         answer("confidence interval"),
         answer("_t_", correct = TRUE),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

### Parameter

Access `parameter` within `tt_height`.

```{r access_parameter, exercise = TRUE, exercise.lines=7}

```
```{r access_parameter-solution}
tt_height$parameter
```

```{r parameter_output, echo=FALSE}
question("What is the parameter's value?",
         answer("66.55556"),
         answer("1.310886"),
         answer("32", correct = TRUE),
         answer("0"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

```{r parameter_name, echo=FALSE}
question("What's it called?",
         answer("_p_"),
         answer("df", correct = TRUE),
         answer("confidence interval"),
         answer("_t_"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```


### `p.value`

Access `p.value` within `tt_height`.

```{r access_p, exercise = TRUE, exercise.lines=7}

```
```{r access_p-solution}
tt_height$p.value
```

```{r p_output, echo=FALSE}
question("What is _p_'s value?",
         answer("66.55556"),
         answer("1.310886"),
         answer("31.2"),
         answer(".20", correct = TRUE),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

```{r p_name, echo=FALSE}
question("What's it called?",
         answer("_p_", correct = TRUE),
         answer("df"),
         answer("confidence interval"),
         answer("_t_"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

### Confidence Interval

([What's a confidence interval?](https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample/introduction-to-confidence-intervals/v/confidence-intervals-and-margin-of-error?modal=1))

Access `conf.int` within `tt_height`.

```{r access_conf, exercise = TRUE, exercise.lines=7, exercise.setup = "prepare-coach_ttest"}

```
```{r access_conf-solution}
tt_height$conf.int
```

```{r conf_output, echo=FALSE}
question("What is the confidence interval for this data set?",
         answer("0.95"),
         answer("[-1.12, 4.98]", correct = TRUE),
         answer("31.2"),
         answer("0.199"),
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

## Graph Results

For a very interesting discussion of why we are not using "dynamite plots", see [_Beware of Dynamite_, an article from the Department of Biostatistics at the Vanderbuilt University School of Medicine](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/TatsukiKoyama/Poster3.pdf).

Instead, let's try a few different options. A simple boxplot is often a good place to start. Use the function `boxplot` and pass it the formula `ht$inches ~ ht$section`. 

```{r t, exercise = TRUE, exercise.lines=8, exercise.setup = "prepare-coach_ttest"}

```
```{r t-solution}
boxplot(ht$inches ~ ht$section, main = "Boxplot of Height in Inches by Section", xlab = "Section", ylab = "Height in Inches")
```

Now go back and edit your code by adding a title. Main should be "Boxplot of Height in Inches by Section". 

Now go back and add the `xlab`, "Section", and the `ylab`, "Height in Inches". 

How about a violin plot? That will tell not only the mean but also the distribution in greater detail than a boxplot can show.

Make sure `ggplot2` is in the library. Then call `ggplot`, passing it the name of the data set (`ht`), then `aes`, which in turn should receive the `x` and `y` variable names. After the call to `ggplot`, put a `+` at the end of the line, then call `geom_violin`, passing it nothing. 

Store what you just did in the variable `p`. 

Finally, look at `p`.

```{r violin, exercise = TRUE, exercise.lines = 15}

```
```{r violin-solution}
p <- ggplot(ht, aes(x=section, y=inches)) + 
  geom_violin()
p
```

Let's add mean and median points to the violin plots. Start with your original code to make `p`, then start with `p` and assign to it itself plus a call to `stat_summary`. Pass to `stat_summary` four things: `fun.y` set to equal `mean`, `geom` equal to "point", `shape` set to 23, and `size` set to 2. 

Then start with the new `p` and add another call to `stat_summary` to it, on this time pass `fun.y = median, geom = "point", size = 2, color = "red"`.

Finally, look at `p`.

```{r violin-mean-and-median, exercise = TRUE, exercise.lines = 8}

```
```{r violin-mean-and-median-solution}
library(ggplot2)
p <- ggplot(ht, aes(x = section, y = inches)) + 
    geom_violin()
p <- p + stat_summary(fun.y = mean,
                      geom = "point",
                      shape = 23, 
                      size = 2)
p <- p + stat_summary(fun.y = median, 
                      geom = "point", 
                      size = 2,
                      color = "red")
p
```

You are amazing!

## I _like_ Math. Where's the Math?

In general, these tutorial-reference lessons are meant for the person who wants to know how properly to use a statistical test without having to get a PhD in statistics. However, some learners enjoy the math behind the tests. This section is dedicated to those learners. Enjoy it!

![](https://www.youtube.com/watch?v=a2rd4Qy8yNI)

![](https://www.youtube.com/watch?v=NkGvw18zlGQ)

And just for fun, a riddle:

![](https://www.youtube.com/watch?v=mmkCS5eA4f8)

## See Also

[One-sample _t_ Test](../one-sample-t-test/)
[Paired _t_ Test](../paired-t-test/)

## References

This lesson is heavily based with thanks on the works of John H. McDonald ([Handbook of Biological Statistics](http://www.biostathandbook.com/chigof.html)) and Salvatore S. Mangiafico ([R Companion to the Biostats Handbook](https://rcompanion.org/rcompanion/b_03.html)). I also copied some wording from the R documentation for the `pwr.2p2n.test` function, written by Stephane Champely <champely@univ-lyon1.fr>, who said, "but this is a mere copy of Peter Dalgaard's work (power.t.test)". 

See also Cohen, J (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum.


