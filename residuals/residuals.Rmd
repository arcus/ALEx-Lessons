---
title: "Residuals"
output: 
 learnr::tutorial:
    progressive: true
    allow_skip: true
    css:
      - www/bootstrap.min.css
      - www/flexdashboard.min.css
      - www/style.css
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(jpeg)
library(psych)
library(HistData)
library(manipulate)
library(UsingR)


tutorial_options(exercise.startover = TRUE,
                 exercise.eval = TRUE, 
                 exercise.cap = "R Code",
                 exercise.completion = TRUE,
                 exercise.diagnostics = TRUE)

options(digits = 3, scipen = 9999)

# For this lesson only

galton <- Galton # comes with the `HistData` package
(fit <- lm(child ~ parent, galton))
```

## Prep

First, take a deep breath and get ready to find the game in all of this.


```{r xkcdlm, echo = FALSE}
knitr::include_graphics("www/xkcdlm.png", 
                          auto_pdf = getOption("knitr.graphics.auto_pdf", 
                                                FALSE), dpi = NULL)
```


Then, if you haven't already, please review the lesson [Introduction to Regression Models](../introduction-to-regression-models/) so you don't feel lost among some of the terms we'll use here.



## What are residuals?

In the graphic below, you have two variables, $\displaystyle X$ and $\displaystyle Y$. 

Let's say $\displaystyle Y$ is parents' average heights, and $\displaystyle X$ is children's heights. Each red dot shows the relationship between those two heights for a single case. 

The purple line is "best" straight line that describes the relationship between the two variables in this graph. It is calculated to go right through the middle of the dots. That's the *regression* or *best fit* line.

Each dot has a vertical distance from that line (those distances are drawn for you in the graph). Those distances are called _residuals_. 

```{r residuals, echo = FALSE}
knitr::include_graphics("www/residuals.png", 
                          auto_pdf = getOption("knitr.graphics.auto_pdf", 
                                                FALSE), dpi = NULL)
```

```{r orientation, echo=FALSE}
question("What is the orientation of the best fit line in the graphic above?",
         answer("Slanted", correct = TRUE),
         answer("Horizontal"),
         answer("Vertical", message = "The vertical lines indicate the residuals."),
         answer("It's not there.", message = "It is there."),
         correct = "You are amazing!",
         incorrect = "No.",
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

In the `galton` data set (which we have loaded for you), we calculate residuals as the distances between actual children's heights and the estimates given by the regression line (which, if you did [the previous lesson](../introduction-to-residuals/), you will already know is accurate only generally, not necessarily for specific children of specific parents).

Since all straight lines everywhere in the universe can be described by two parameters, a **slope** and an **intercept**, we'll use *least squares criteria* to provide, for any line, **two equations in two unknowns** so we can solve for these parameters, the slope and the intercept.

Huh?

This just means we're going to get clever and figure out how to find the slope and the intercept of any line we're interested in. That way we'll know all about the line (provided it's a *straight* line---but let's not go there yet).

```{r allaboutit, echo=FALSE}
question("Knowing all about the line means we'll know all about the data.",
         answer("False", correct = TRUE),
         answer("True", message = "A line in this context only approximates a relationship between two scalar variables. We lose a lot of information when we reduce rich data points to a line."),
         correct = "Excellent!",
         incorrect = "Bzzzzt!",
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

## You just mentioned equations. What equations?

```{r residuals2, echo = FALSE}
knitr::include_graphics("www/miniresiduals.png", 
                          auto_pdf = getOption("knitr.graphics.auto_pdf", 
                                                FALSE), dpi = NULL)
```

>The first equation says that the "errors" in our estimates, the residuals, have mean zero. 

In other words, the residuals are "balanced" among the data points; they're just as likely to be positive as negative. 

If you added up all the distances of the vertical lines from the regression line in the graph in the previous section, where distances below the regression line are negative and the distances above the regression line are positive, the total would be zero. That's because the regression line was created _so that_ this would be true. That's how to make it the best fit line.

```{r whyisitzero, echo=FALSE}
question("Why is the mean of all the residuals around a best fit line 0?",
         answer("Because if the mean wasn't zero, it would be proof that we don't have a true best fit line.", correct = TRUE),
         answer("Because the predictors correlate with the best fit line.", message = "In fact, the predictors do _not_ correlate with the best fit line."),
         answer("Because otherwise, the line might be curved.", message = "You can also make a curved best fit line for which the residuals are 0. We won't cover that here."),
         correct = "Excellent!",
         incorrect = "No.",
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```


>The second equation says that our residuals must be uncorrelated with our predictors, the parentsâ€™ height. 

This makes sense---if the residuals and predictor (in this case, parents' heights) were correlated, then you could make a *better* prediction and reduce the distances (residuals) between the actual outcomes and the predictions. But we already have the best line, right? So there can't be a correlation between the residuals and the predictor, parent's heights: or, more generally, between predictors and the best fit line. 

```{r whynocorr, echo=FALSE}
question("Why doesn't the predictor correlate with the residuals?",
         answer("Because if it did, that would be proof that we could make a line that fits better than the one that defines the sum of the residuals to be 0.", correct = TRUE),
         answer("Because the predictor doesn't predict the residuals; it predicts the dependent variable, which is in this case children's heights.", correct = TRUE),
         answer("Because residuals are wildly random.", message = "They're not random. Their sums are 0."),
         answer("This whole discussion makes no sense.", message = "Maybe go back to the beginning of this lesson and try again. Or go back even further, to the lesson [Introduction to Regression Models](../introduction-to-regression-models/). If you don't know how to code in R, use [the ALEx curriculum generator](https://redcap.chop.edu/surveys/?s=FYA9THYWHH) to create a curriculum for yourself that you can use to fill in any missing skills. You can also contact me at braunsb@email.chop.edu and I will help you, then probably add a correction or two to this lesson to make it easier to understand."),
     correct = "Excellent!",
     incorrect = "There are two correct answers.",
     random_answer_order = TRUE,
     allow_retry = TRUE
)
```

## Equations in Action

Let's write some code to check these concepts. First, generate the regression line of `child` across `parent` and call it `fit`. Use the R function `lm`. Recall that, by default, its first argument is a formula (such as `child ~ parent`) and its second argument is the dataset, in this case, `galton`. 

```{r generateregrline, exercise = TRUE, exercise.lines = 8}

```
```{r generateregrline-hint-1}
fit <- ?
```
```{r generateregrline-hint-2}
fit <- lm(...)
```
```{r generateregrline-hint-3}
fit <- lm(..., galton)
```
```{r generateregrline-solution}
fit <- lm(child ~ parent, galton)
```


Now we'll examine `fit` to see its slope and intercept. The residuals we're interested in are stored in the  928-long vector `fit$residuals`. If you type `fit$residuals` (which you can do in the box below, if you like), you'll see a lot of numbers scroll by, which isn't very useful; however if you  type `summary(fit)` you will see a more concise display of the regression data. Do this now.

```{r seefit, exercise = TRUE, exercise.lines = 5}

```
```{r seefit-hint}
summary(...)
```
```{r seefit-solution}
summary(fit)
```

So `lm` calculated the residual for every data point, and `summary` provides us with information about this particular group of residuals, which we stored in `fit`. 

What else is in `fit`? Try typing `names(fit)` to find out.

```{r whatinfit, exercise = TRUE, exercise.lines = 8}

```
```{r whatinfit-hint-1}
names(...)
```
```{r whatinfit-solution}
names(fit)
```

That's a lot of information! But we aren't going to concern ourselves with all of it right now. We're just interested in the residuals. But isn't it cool how R creates a data set (`fit`) out of running `lm` on a dataset? It's one of the reasons so many people like R. 

Back to the residuals. 

First check the mean of `fit$residuals` to see if it's close to 0.

```{r checkmeanfit, exercise = TRUE, exercise.lines = 5}

```
```{r checkmeanfit-hint}
mean(...)
```
```{r checkmeanfit-solution}
mean(fit$residuals)
```

That's close enough to 0 for any difference to be due to rounding errors. 

Now we'll use the `cov` function to check the correlation between the residuals and the predictor (to find out more about `cov`, type `?cov` at the command prompt in any code box). 

Remember, we said above that there should be no correlation between predictor and residuals. That means that the result should be close to 0.

Pass `cov` two arguments: the residuals that are stored in `fit` and the predictor variable that is in the `galton` dataset. 

```{r covrespred, exercise = TRUE, exercise.lines = 8}

```
```{r covrespred-hint-1}
cov(..., ...)
```
```{r covrespred-hint-2}
cov(fit$residuals, ...)
```
```{r covrespred-solution}
cov(fit$residuals, galton$parent)
```

Again, close enough to 0 for any difference from it to be due to rounding. 

We've defined for you two R functions, `est` and `sqe`. Both take two inputs, a _slope_ and an _intercept_. The function `est` calculates a child's height (the y-coordinate) using the line defined by the two parameters, (slope and intercept), and the parents' heights in the Galton data as x-coordinates.  

Let `mch` represent the mean of the `galton` childrens' heights and `mph` the mean of the galton parents' heights. Let `ic` and `slope` represent the intercept and slope of the regression line respectively. the point (`mph`,`mch`) lies on the regression line.

```{r thismeans, echo=FALSE}
question("This means that",
         answer("`mch = ic + slope * mph`", correct = TRUE),
         answer("`mph = ic + slope * mch`"),
         answer("I haven't the slightest idea.", message = "Maybe go back to the beginning of this lesson and try again. Or go back even further, to the lesson [Introduction to Regression Models](../introduction-to-regression-models/). If you don't know how to code in R, use [the ALEx curriculum generator](https://redcap.chop.edu/surveys/?s=FYA9THYWHH) to create a curriculum for yourself that you can use to fill in any missing skills. You can also contact me at braunsb@email.chop.edu and I will help you, then probably add a correction or two to this lesson to make it easier to understand."),
         correct = "You are amazing!",
         incorrect = "Not quite.",
         random_answer_order = TRUE,
         allow_retry = TRUE
)
```

- Class: mult_question
 This means
  AnswerChoices: $\displaystyle $; ; I haven't the slightest idea.
  CorrectAnswer: mch = ic + slope * mph
  AnswerTests: omnitest(correctVal='mch = ic + slope*mph')
  Hint: A line is the set of all points (x,y) satisfying the equation y = mx + b, where m is the slope of the line and b is its intercept. Remember that the point (mph,mch) lies on the regression line with intercept ic and slope "slope".


The function sqe calculates the sum of the squared residuals, the differences between the actual children's heights and the estimated heights specified by the line defined by the given parameters (slope and intercept).  R provides the function deviance to do exactly this using a fitted model (e.g., fit) as its argument. However, we provide sqe because we'll use it to test regression lines different from fit. 


We'll see that when we vary or tweak the slope and intercept values of the regression line which are stored in fit$coef, the resulting squared residuals are approximately equal to the sum of two sums of squares - that of the original regression residuals and that of the tweaks themselves. More precisely, up to numerical error, 


sqe(ols.slope+sl,ols.intercept+ic) == deviance(fit) + sum(est(sl,ic)Ë†2 )


Equivalently, sqe(ols.slope+sl,ols.intercept+ic) == sqe(ols.slope, ols.intercept) + sum(est(sl,ic)Ë†2 )



The left side of the equation represents the squared residuals of a new line, the "tweaked" regression line. The terms "sl" and "ic" represent the variations in the slope and intercept respectively. The right side has two terms. The first represents the squared residuals of the original regression line and the second is the sum of squares of the variations themselves. 


We'll demonstrate this now. First extract the intercept from fit$coef and put it in a variable called ols.ic . The intercept is the first element in the fit$coef vector, that is fit$coef[1].
  CorrectAnswer: ols.ic <- fit$coef[1]
  AnswerTests: omnitest(correctExpr='ols.ic <- fit$coef[1]')
  Hint: Type "ols.ic <- fit$coef[1]" at the R prompt.


Now extract the slope from fit$coef and put it in the variable ols.slope; the slope is the second element in the fit$coef vector, fit$coef[2].
  CorrectAnswer: ols.slope <- fit$coef[2]
  AnswerTests: omnitest(correctExpr='ols.slope <- fit$coef[2]')
  Hint: Type "ols.slope <- fit$coef[2]" at the R prompt.

- Class: figure
Now we'll show you some R code which generates the left and right sides of this equation.  Take a moment to look it over. We've formed two 6-long vectors of variations, one for the slope and one for the intercept. Then we have two "for" loops to generate the two sides of the equation.
  Figure: demofile.R
  FigureType: new


Subtract the right side, the vector rhs, from the left, the vector lhs, to see the relationship between them. You should get a vector of very small, almost 0, numbers.
  CorrectAnswer: lhs-rhs
  AnswerTests: omnitest(correctExpr='lhs-rhs')
  Hint: Type "lhs-rhs" at the R prompt.


You could also use the R function all.equal with lhs and rhs as arguments to test for equality. Try it now.
  CorrectAnswer: all.equal(lhs,rhs)
  AnswerTests: ANY_of_exprs('all.equal(lhs,rhs)','all.equal(rhs,lhs)')
  Hint: Type "all.equal(lhs,rhs)" at the R prompt.


Now we'll show that the variance in the children's heights is the sum of the variance in the OLS estimates and the variance in the OLS residuals. First use the R function var to calculate the variance in the children's heights and store it in the variable varChild.
  CorrectAnswer: varChild <- var(galton$child)
  AnswerTests: omnitest(correctExpr='varChild <- var(galton$child)')
  Hint: Type "varChild <- var(galton$child)" at the R prompt.


Remember that we've calculated the residuals and they're stored in fit$residuals. Use the R function var to calculate the variance in these residuals now and store it in the variable varRes.
  CorrectAnswer: varRes <- var(fit$residuals)
  AnswerTests: omnitest(correctExpr='varRes <- var(fit$residuals)')
  Hint: Type "varRes <- var(fit$residuals)" at the R prompt.


Recall that the function "est" calculates the estimates (y-coordinates) of values along the regression line defined by the variables "ols.slope" and "ols.ic". Compute the variance in the estimates and store it in the variable varEst.
  CorrectAnswer: varEst <- var(est(ols.slope, ols.ic))
  AnswerTests: omnitest(correctExpr='varEst <- var(est(ols.slope, ols.ic))')
  Hint: Type "varEst <- var(est(ols.slope, ols.ic))" at the R prompt.


Now use the function all.equal to compare varChild and the sum of varRes and varEst.
  CorrectAnswer: all.equal(varChild,varEst+varRes)
  AnswerTests: ANY_of_exprs('all.equal(varChild,varEst+varRes)','all.equal(varEst+varRes,varChild)','all.equal(varChild,varRes+varEst)','all.equal(varRes+varEst,varChild)')
  Hint: Type "all.equal(varChild,varEst+varRes)" at the R prompt.



Since variances are sums of squares (and hence always positive), this equation which we've just demonstrated,  var(data)=var(estimate)+var(residuals), shows that the variance of the estimate is ALWAYS less than the variance of the data.

- Class: mult_question
Since  var(data)=var(estimate)+var(residuals) and variances are always positive,  the variance of residuals
  AnswerChoices: is less than the variance of data; is greater than the variance of data; is unknown without actual data
  CorrectAnswer: is less than variance of data
  AnswerTests: omnitest(correctVal='is less than the variance of data')
  Hint: The equation says var(residuals)=var(data)-var(estimate); we're subtracting a positive number from var(data) to give us var(residuals)



The two properties of the residuals we've emphasized here can be applied to datasets which have multiple predictors. In this lesson we've loaded the dataset attenu which gives data for 23 earthquakes in California. Accelerations are estimated based on two predictors, distance and magnitude. 



Generate the regression line for this data. Type efit <- lm(accel ~ mag+dist, attenu) at the R prompt.
  CorrectAnswer: efit <- lm(accel ~ mag+dist, attenu)
  AnswerTests: creates_lm_model('efit <- lm(accel ~ mag+dist, attenu)')
  Hint: Type "efit <- lm(accel ~ mag+dist, attenu)" at the R prompt.


Verify the mean of the residuals is 0.
  CorrectAnswer: mean(efit$residuals)
  AnswerTests: omnitest(correctExpr='mean(efit$residuals)')
  Hint: Type "mean(efit$residuals)" at the R prompt.


Using the R function cov verify the residuals are uncorrelated with the magnitude predictor, attenu$mag.
  CorrectAnswer: cov(efit$residuals, attenu$mag)
  AnswerTests: ANY_of_exprs('cov(efit$residuals, attenu$mag)','cov(attenu$mag,efit$residuals)')
  Hint: Type "cov(efit$residuals, attenu$mag)" at the R prompt.


Using the R function cov verify the residuals are uncorrelated with the distance predictor, attenu$dist.
  CorrectAnswer: cov(efit$residuals, attenu$dist)
  AnswerTests: ANY_of_exprs('cov(efit$residuals, attenu$dist)','cov(attenu$dist,efit$residuals)')
  Hint: Type "cov(efit$residuals, attenu$dist)" at the R prompt.


Congrats! You've finished the course on Residuals. We hope it hasn't left a bad taste in your mouth.

- Class: mult_question
"Would you like to receive credit for completing this course on
    Coursera.org?"
  CorrectAnswer: NULL
  AnswerChoices: Yes;No
  AnswerTests: coursera_on_demand()
  Hint: ""

## Contributions

Slides to accompany this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson is based on Regression_Models/01_03_ols. Galton data is from John Verzani's website, [http://wiener.math.csi.cuny.edu/UsingR/](http://wiener.math.csi.cuny.edu/UsingR/).

[Brian Caffo, Jeff Leek, Roger Peng, Nick Carchedi, Sean Kross,](https://github.com/DataScienceSpecialization/courses#contributors) and Sheila Braun

Galton F : Regression towards mediocrity in hereditary stature. Journal of the anthropological institute 1886; 15: 246â€“263.

[http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html](http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html)

[Predicting height: the Victorian approach beats modern genomics](http://www.wired.com/wiredscience/2009/03/predicting-height-the-victorian-approach-beats-modern-genomics/)


## License

These materials are available under the Creative Commons Attribution NonCommercial ShareAlike (CC-NC-SA) license (http://www.tldrlegal.com/l/CC-NC-SA).


## Math


As shown algebraically in the slides, the equations for the intercept and slope are found by supposing a change is made to the intercept and slope. Squaring out the resulting expressions produces three summations. The first sum is the original term squared, before the slope and intercept were changed. The third sum totals the squared changes themselves. For instance, if we had changed fitâ€™s intercept by adding 2, the third sum would be the total of 928 4â€™s. The middle sum is guaranteed to be zero precisely when the two equations (the conditions on the residuals) are satisfied.